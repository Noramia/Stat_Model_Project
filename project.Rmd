---
title: "SM3_Project"
author: "Shingyan Kwong, Mohammad Rezai, Xiaomeng Gu, Wenjun Liu and Oliver Lountain"
date: "June 3, 2018"
output:
  pdf_document: default
  html_document: default
header-includes:
- \usepackage{float}
- \floatplacement{figure}{H}
---
```{r,echo=FALSE, message = F}
library(broom)
library(MASS)
library(magrittr)
library(dplyr)
library(broom)
library(ggplot2)
library(GGally)
library(corrplot)
#setwd("C:/Users/Oliver/Documents/GitHub/Stat_Model_Project")
```



#Part A.

#1. 
```{r,echo=FALSE}
child<-read.table("Child_Height.txt",header=T)
```

Here, we produce pairwise scatterplots for height, weight and length to see the individual relationships between these varaibles (Figure 1):

```{r, echo = F, fig.cap = "Pairwise Scatterplots for Height, Weight and Length"}
pairs(~Height+Weight+Length,data=child)
```

In figure 1 we see that there is evidence of a strong, positive linear relationship between  length and the two predictor variables, height and weight. The associated correlation coefficients are 0.881 and 0.894 respectively. There is also a strong, positive linear relationship between height and weight. This suggests that the two predictors may be dependent one another.


#2. 

Here we fit three linear models: lm1 which fits Length against Height and Weight; lm2 which fits Length against Height; and lm3 which fits Length against Weight.

```{r}
lm1<-lm(Length~Height+Weight, data=child)
lm2<-lm(Length~Height, data=child)
lm3<-lm(Length~Weight, data=child)

```


#3. 

The model assumptions which may be checked via diagnostic plots are as follows. 

Linearity: Check the residuals vs fitted and the residuals vs predictor plots. Linearity is reasonable if random scatter above and below the 0 line is observed. 

Constant Variance: Check scale location plot. Homoscedacity is reasonable if constant variance of residuals is observed across the scale location plot. 

Normality: Check normal quantile-quantile plot. Normality is reasonable if most points between -2 and 2 are on/close to the diagonal line. 

#4. 

#Full model (lm1)

```{r, echo = F, fig.cap = "Residuals plots for Full Model"}
par(mfrow=c(2,2))
plot(lm1)
```

```{r, echo = F, fig.cap = "Residuals plots for Height and Weight Individually (Full Model)"}
par(mfrow=c(1,2))
res1<-rstudent(lm1)
fit<-fitted(lm1)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")

```

From figures 2 and 3 we can make the following assessments:

**Linearity:** Given the small number of data points available, roughly random scatter is observed in the residual vs fitted and residual vs predictor plots. There is a couple of high residual points but it is not too bad. Linearity is reasonable. 

**Constant variance:** Scale location plots appear to show heteroscedacity. Constant variance is not reasonable.  

**Normality:** There is some minor departure from normality in the beginning and the tails of the standardized residuals. Overall the points are fairly close to the diagonal line. Normality is reasonable. 

**Leverage:** There are 2 data points in the zone of danger. These high leverage points are having a disproportionate effect on the model. 


#Height Model (lm2)

```{r, echo = F, fig.cap = "Residuals plots for Height Model"}
par(mfrow=c(2,2))
plot(lm2)
```

```{r, echo = F, fig.cap = "Residuals plots for Height and Weight Individually (Height Model)"}
par(mfrow=c(1,2))
res1<-rstudent(lm2)
fit<-fitted(lm2)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")
```

From figures 4 and 5 we can make the following assessments:

**Linearity:** Non-random scatter observed in residual vs fitted and residual vs predictor plots. Linearity is not reasonable. 

**Constant Variance:** Variance appears to increase for the middle fitted values and then decrease again. Constant variance is not reasonable. 

**Normality:** There are several points deviating from the diagonal line on the normal quantile-quantile plot. Normality is not reasonable. 

**Leverage:** There is one point with high leverage. 


#Weight Model (lm3)

```{r, echo = F, fig.cap = "Residuals plots for Weight Model"}
par(mfrow=c(2,2))
plot(lm3)
```

```{r, echo = F, fig.cap = "Residuals plots for Height and Weight Individually (Weight Model)"}
par(mfrow=c(1,2))
res1<-rstudent(lm3)
fit<-fitted(lm3)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")

```

From figure 6 and 7 we can make the following assessments:

**Linearity:** Rough random scatter in observed in residual vs fitted and residual vs predictor plots. The fitted plot shows some evidence of curvature but overall it is acceptable. Linearity is reasonable. 

**Constant Variance:** Variance is roughly constant across the scale location plot. Constant variance is reasonable. 

**Normality:** Most points are close to the diagonal line except 2. Normality is reasonable. 

**Leverage:** There is one data point with high leverage. 

\newpage

#5.

Here we make a comparison of the three models.


We first obtain summaries for each of the three models:

\hfill

**Full Model:**
```{r, echo = F}
summary(lm1)
```

\hfill

**Height Model:**
```{r, echo = F}
summary(lm2)
```

\newpage

**Weight Model:**
```{r, echo = F}
summary(lm3)
```


###(a)

In the full model, neither predictor variables is statistically significant (at the 0.05 level), and the numerical values of the two coefficients are both smaller than those of the single predictor models.

###(b)

**Full model:**

Holding height constant, the full model predicts that an increase of of 1kg will on average increase the length of the cathetar by 0.42081cm. 

**Weight only model:**

Without regard for height, this model predicts that an increase of 1kg will on average increase the cathetar length by 0.61136cm. 

\newpage

#6

###(a)

First, we construct the model matrices for the height only and weight only models. 

```{r,echo=FALSE}
M2 <- model.matrix(lm2)
M3 <-  model.matrix(lm3)
```

Model Matrix for lm2:
```{r, echo = F}
M2
```

Model matrix for lm3:
```{r, echo = F}
M3
```

Here, we define $\mathbf{1} := (1,1,1,1,1,1,1,1,1,1,1,1)$, the vector of intercepts for both models. We also denote the vector of height values by $\mathbf{x}_1$ and the vector of weight values by $\mathbf{x}_2$. Then we find that:

$\mathcal{L}_1$ is the space spanned by the columns of M2, that is $\mathcal{L}_1 = span \{ \mathbf{1}, \mathbf{x}_1 \}$

$\mathcal{L}_2$ is the space spanned by the columns of M3, that is $\mathcal{L}_2 = span \{ \mathbf{1}, \mathbf{x}_2 \}$

Then, the intersection of the two subspaces is the intercept column, that is $\mathcal{L}_1 \cap \mathcal{L}_2 = span { \{\mathbf{1} \} }.$

###(b)

We note that $(\mathcal{L}_1 \cap \mathcal{L}_2)^{\perp}$ is the subspace of all vectors orthogonal to $\mathbf{1}$. Then, in order to find the intersections of $\mathcal{L}_1$ and $\mathcal{L}_2$ with $(\mathcal{L}_1 \cap \mathcal{L}_2)^{\perp}$, we first find orthonormal bases for $\mathcal{L}_1$ and $\mathcal{L}_2$.

We achive this by applying the Gram-Schmidt process. First, we define the basis vectors for both subspaces, and a function norm_vec to find the norm of a vector: 

```{r}
one <- c(1,1,1,1,1,1,1,1,1,1,1,1) # intercept vector
x1 <- M2[,2] # vector of height values
x2 <- M3[,2] # vector of weight values
norm_vec <- function(x) sqrt(as.numeric(t(x) %*% x))
```

Next, we find an orthonormal basis for $\mathcal{L}_1$:

```{r}
v1 <- one / norm_vec(one)
v2_ <- x1 - as.numeric((t(x1) %*% v1)) * v1
v2 <- v2_ / norm_vec(v2_)
```

Now

$$\mathbf{v}_1 = \frac{1}{\sqrt{12}}(1,1,1,1,1,1,1,1,1,1,1,1);$$
$$\mathbf{v}_2 = (0.0616, 0.5846, -0.0722, -0.0217, 0.1299, -0.0469, 0.0667, -0.4511, -0.0848, -0.4259, -0.1859, 0.4457)$$

Then $\mathcal{L}_1 = span \{\mathbf{v}1, \mathbf{v}2\}$.

Now, an orthonormal basis for $\mathcal{L}_2$:

```{r}
w1 <- one / norm_vec(one)
w2_ <- x2 - as.numeric((t(x2) %*% w1)) * w1
w2 <- w2_ / norm_vec(w2_)
```

Now

$$\mathbf{w}_1 = \frac{1}{\sqrt{12}}(1,1,1,1,1,1,1,1,1,1,1,1);$$ 
$$\mathbf{w}_2 = (0.0216, 0.6408, -0.0304, -0.0940, 0.1606, -0.2445, 0.0043, -0.3427, -0.0593, -0.3312, -0.1981, 0.4729)$$


Then $\mathcal{L}_2 = span \{\mathbf{w}1, \mathbf{w}2\}$.

Now, we have that $\mathbf{v}_1 = \mathbf{w}_1$ is parallel to $\mathbf{1}$, and that $\mathbf{v}_2$ and $\mathbf{w}_2$ are orthogonal to $\mathbf{1}$, that is, $\mathbf{v}_2, \mathbf{w}_2 \in ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$.

As a result, we find that:

$$ \mathcal{L}_1 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp} = span \{ \mathbf{v}_2 \}; $$

$$ \mathcal{L}_2 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp} = span \{ \mathbf{w}_2 \}. $$



###(c)

Given that both $\mathcal{L}_1 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$ and $\mathcal{L}_2 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$ are one-dimensional subspace, we can compute the angle between them using the relation:

$$
\cos \theta = \frac{ \langle \mathbf{u}, \mathbf{v} \rangle }{||\mathbf{u}|| \; ||\mathbf{v}||}
$$

Where $\mathbf{u}$ and $\mathbf{v}$ are two vectors and $\theta$ is the angle between them.

\newpage

We compute the angle between the two spaces in **(b)** as follows:

```{r}
inner <- t(v2) %*% (w2) #Defining the inner product of v2 and w2
norm_v2 <- norm_vec(v2)
norm_w2 <- norm_vec(w2)
theta <- acos(inner/(norm_v2 * norm_w2)) #Finding the angle between v2 and w2
```

Now we have that the angle between $\mathbf{v}_2$ and$\mathbf{w}_2$ is given by:

```{r, echo = F}
theta
```

The angle is not $\pi$ indicating that the two spaces are not orthogonal. This suggests that height and weight are not independent. In fact they are fairly correlated. 

#7.

In this section we decide upon a final model for the data. We note that since the subspaces $\mathcal{L}_1 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$ and $\mathcal{L}_2 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$ are not orthogonal, the weight and height predictors are not independent; more specifially, they have a significant correlation of 0.961. As such, the full model in which Length was fitted against both Height and Weight should not be used. Now, what remains is to decide between the individual height and weight models. In figure 4, we see that the assumptions of linearity, constant variance and normality fail in the height model, and in figure 6 it is clear that these assumptions are much more reasonanble in the weight model. As a result, we can conclude that the most appropriate model for the data is lm3, the model in which length is fit against height alone.


\newpage

# Part B


## Introduction

In this section we obtain a predictive model for mammographic mass severity, a measure of the status of mammographic mass lesions, on a scale from 0 to 1, where 0 is assigned to a benign tumor, and 1 is assigned to a malignant tumor. Interest in this analysis arises from there being a low predicitve value of breast biopsy from mammograms. This low predictive value has been found to lead to approximately 70% of unnessessary biopsies of benign tumors. Analysis is performed on the dataset "mammo", containing the true status of 961 mammographic mass lesions, with the response variable severity as described. Four response variables are considered:

**Age** - the patient's age in years;

**Shape** - a factor variable with four levels: 1 for round, 2 for oval, 3 for lobular, and 4 for irregular;

**Margin** - a factor varaible with five levels: 1 for circumscribed, 2 for microlobulated, 3 for obscured, 4 for ill-defined, and 5 for spiculated;

**Density** - a factor with four levels: 1 for high, 2 for iso, 3 for low, and 4 for fat-containing.

The aim of this study is to produce the best predictive model for mammogram mass severity. The model must be parsimonious whilst having minimun residual error. Predicted probabilities of severity may also be generated from the model. This information may 
be used by clinicians to determine whether a biopsy is appropriate for the patient. 


**This introduction should probably be reworked but I this hope is a good starting point**


##Data Entry and Cleaning
First, we enter the data and define any values which are assigned question marks to be missing values. 

```{r,echo=F}
mammo <- read.csv("mammo.txt", header=TRUE, na.strings = "?")
```

We then note that BI.RADS is not a predictor variable, and remove it from our analysis:

```{r,echo=F}
mammo <- dplyr::select(mammo, Age, Shape, Margin, Density, Severity)
```

We generate a correlation matrix and a pairwise scatterplot to observe any relationships between the predictor variables and the response. It was necessary to omit observations with NA values. Shape and Margin were included as increases in their indices are associated with a greater risk of cancer. Density was omitted as the index is not associated with a greater risk. Thus correlation between Density and Severity would not be statistically meaningful.  

```{r,echo=F,warning=F,message=F,fig.cap = "Correlation matrix depicting the relationship between Margin, Shape, Severity and Age."}
library(corrplot)
corMat<-cor(na.omit(dplyr::select(mammo, Age, Shape, Margin,Severity)))
corrplot(corMat, type="upper", order="hclust",tl.col="black",tl.srt = 45 )
```

```{r, fig.cap = "Comparison Matrix for Mammo Data", warning = F, echo = F}
ggpairs(mammo)
```

From figure 9 we can make the following observations:

Mild positive correlation is observed between Age and the other variables. There is a correlation are 0.364 and 0.411 for Shape and Age and Margin and Age respectively.

Moderate positive correlation is observed between Severity and the other 3 predictors. 

Strong positive correlation between Shape and Margin with a correlation of 0.742. 


\newpage


We can now check the variable types for the data:

```{r,echo=FALSE}
str(mammo)
```

We note that Shape, Margin, Density and Severity should all be factor variables, and as such convert them:

```{r,echo=FALSE}
mammo$Shape <- as.factor(mammo$Shape)
mammo$Margin <- as.factor(mammo$Margin)
mammo$Density <- as.factor(mammo$Density)
mammo$Severity <- as.factor(mammo$Severity)
```

We now see that all of the data types are correct:

```{r,echo=FALSE}
str(mammo)
```


## Data Visualisations and Data Summaries

To visualise the data, we first produce summary statisitics for the dataset as a whole, and for each individual variable:

```{r,echo=FALSE}
summary(mammo$Age)
summary(mammo$Shape)
summary(mammo$Margin)
summary(mammo$Density)
summary(mammo$Severity)
```


We can then generate boxplots of Age against the other four predictors to see how the different levels of the factor variables are affected by a patients age.

\newpage

First, for Severity (Figure 10):

```{r, fig.cap = "Boxplot of Age Against Severity", echo = F, warning = F}
plot(mammo$Age~mammo$Severity)
```

In figure 10 we note that in general, a severity value of 1 is correlated with higher ages than that of 0. In other words, patients with malignant tumors tend to be older than patients with benign tumors Also, the interquartile range box for severity = 0 tends to be larger than severity = 1. We also note that there is an outlier at a low age (approximately 30) when severity = 1.


\newpage

Then, for Shape (Figure 11):

```{r, fig.cap = "Boxplot of Age Against Shape", echo = F, warning = F}
plot(mammo$Age~mammo$Shape)
```

In figure 11 we see that the median ages for patient with round and oval shapes are approximately equal. Also, the boxplot for patients with lobular shape is left skewed. There is a outlier in the boxplot with round shape, and one in the boxplot with irregular shape. 


\newpage


Next, for Margin (Figure 12):

```{r, fig.cap = "Boxplot of Age Against Margin", echo = F, warning = F}
plot(mammo$Age~mammo$Margin)
```


In figure 12 it can be seen that the median age for patients who have obscured, ill-defined and spiculated margin is approximately equal. Moreover, the interquartile range for microlobulated margin is larger than for others. There is a outlier in ill-defined margin.


\newpage

Finally, for Density (Figure 13):

```{r, fig.cap = "Boxplot of Age Against Margin", echo = F, warning = F}
plot(mammo$Age~mammo$Density)
```

In figure 13 we note that the boxplots for high and low density are left-skewed, and the boxplot for iso density is right-skewed. Although the median age for low and fat-containing density is similar, the age of low density group is more variable than the other. Moreover, the range from the lowest age to the highest age for low density is larger than  others.




We can also create bar graphs to visualise how the number of patients with different values for each of the three factor predictor variables is distributed with respect to their Severity value.

\newpage

First, we consider Shape (Figure 14):

```{r, fig.cap = "Bar Graph of Severity Against Shape", echo = F, warning = F}
ggplot(mammo, aes(x=mammo$Severity))+geom_bar(aes(fill=mammo$Shape))
```

In figure 14 it can be seen that patients who have benign tumors tend to have more round shapes and oval shapes than patients who have malignant ones.  Furthermore, patients who have benign tumors tend to have less irregular shape than patients who have malignant tumors. There is an approximately equal number of patients that have lobular shape between patients who have malignant tumors and those who do not.  


\newpage

Next, we consider Margin (Figure 15):

```{r, fig.cap = "Bar Graph of Severity Against Margin", echo = F, warning = F}
ggplot(mammo, aes(x=mammo$Severity))+geom_bar(aes(fill=mammo$Margin))
```

We see in figure 15 that ptients who have benign tumors tend to have more circumscribed margin than those whose tumors are malignant. Moreover, patients who have non-malignant tumors tend to have less microlobulated, obscured, ill-defined, and spiculated margin than patients who have malignant ones.


\newpage

Finally, we consider Density (Figure 16):

```{r, fig.cap = "Bar Graph of Severity Against Density", echo = F, warning = F}
ggplot(mammo, aes(x=mammo$Severity))+geom_bar(aes(fill=mammo$Density))
```

In figure 16 we see that patients who have non-malignant tumors tend to have more iso and low density. In addition, these two groups (severity=1 and severity=0) tend to have equal number of patients with high and fat-containing density.




## Model Fitting and Model Selection

We now fit a logistic linear model (full.glm) to the data, with Severity as the response variable, and Age, Shape, Margin and Density as the predictor variables with interaction terms up to second order. To achieve a parsimony, iInsignificant terms (p-value>0.05) were removed by the backwards selection algorithmn. Our analysis showed that all the interactions terms and Density are not statistically significant at the 0.05. Details of each step of the model selection process are available in the Part B Appendix under "Model Fitting and Model Selection."

```{r, warning = F,echo=F, include = F}
full.glm <- glm(Severity ~ (Age+Shape+Margin+Density)^2, data = mammo, family = "binomial")
```


```{r, warning = F,echo=F, include = F}
back.glm <- update(full.glm, .~. - Margin:Density)
```


```{r, warning = F,echo=F, include = F}
back.glm <- update(back.glm, .~. - Shape:Density)
```


```{r, warning = F,echo=F, include = F}
back.glm <- update(back.glm, .~. - Age:Margin)
```


```{r, warning = F,echo=F, include = F}
back.glm <- update(back.glm, .~. - Shape:Margin)
```


```{r, warning = F,echo=F, include = F}
back.glm <- update(back.glm, .~. - Age:Density)
```


```{r, warning = F,echo=F, include = F}
back.glm <- update(back.glm, .~. - Age:Shape)
```


```{r, warning = F,echo=F, include = F}
back.glm <- update(back.glm, .~. - Density)
```


```{r, warning = F,echo=F}
final.glm <- back.glm
```

\newpage

## Justification of the final model

The following is the proposed final model. 

```{r,echo=F}
tidy(final.glm)
```

This model was obtained by first starting with a saturated model with all the two way interaction terms. Statistically insiginificant terms were removed via the backwards selection algorithmn at the 0.05 significance level. Hence the final model is the most parsimonious model with all the statistically significant predictors. 

## Interpretation of Parameters

Intercept: A woman of Age=0 with a mammogram of Shape=1 (round), Margin=1 (circumscribed) has log-odds=-4.7195 of having a malignant tumour. 

Age: Holding all other variables constant, a one year increase in age increases the log-odds of having a malignant tumour by 0.05388. 

Shape: Holding all other variables constant, having a lesion of Shape=2 (oval) increases the log-odds of having a malignant tumour by -.4478 compared to Shape=1 (round). For Shape=2 (lobular), th increase is 0.4992 and for Shape=3 (irregular), the increase is 1.2428, all relative to Shape=1.  

Margin: Holding all other variables constant, having a lesion of Margin=2 (microlobulated) increases the log-odds of a malignant tumour by 1.5829 comapred to Margin=1 (circumscribed). For Margin =3 (obscured) the change in log-odds is 1.2631, Margin=4 (ill-defined) 1.5432 and Margin=5 (spiculated) 2.0321, all relative to Margin=1. 


##Predicting Probabilities and Interpretation

In this section, we use our final model (final.glm) to predict the probability of a specific patient, that is, a patient with given values for each of the predictor varaibles. Given that the response variable is defined to be 0 for benign (not cancerous) and 1 for malignant (cancerous), the fitted values lie between 0 and 1 and hence predict the  probability for a given patient to have a malignant tumor.

We first fit the probabilties of each datapoint in the dataset based on the final model:

```{r,echo=F}
probabilities <- fitted(final.glm)
summary(probabilities)
```

We can produce a histogram to visualise the overall distribution of probabilities (Figure 17):

```{r, fig.cap = "Histogram of Fitted Probabilities", echo = F}
hist(probabilities)
```

Here we note that in general, it appears that most patients are either very likely, or very unlikely to have a malignant tumor. As a result, we might expect when predicting probabilities, that is most cases the predictions will be either very high or very low.

We can produce plots to visualise the probabilities for different levels of the predictor variables.

We first define a modified version of the mamo data, including only Age, Shape, Margin and Severity, and ignoring the missing values in order to be able to create valid plots:

```{r,echo=F}
newMammo <- mammo %>% select(Age, Margin, Shape, Severity)
newMammo <- na.omit(newMammo)
```


We can now create plots of probabilities against Age, Shape and Margin (Figures 18, 19 and 20 respectively):

```{r, fig.cap = "Probabilities against Age",echo=F}
plot(probabilities ~ newMammo$Age)
```

Here we see that there appears to be a weak, positive relationship between age and the probability of having a malignant tumor, and it is difficult to say whether the relationship is linear or not.

```{r, fig.cap = "Probabilities against Shape",echo=F}
boxplot(probabilities ~ newMammo$Shape)
```

Here we see that as shape tends from the round, regular shape to a more irregular one, the predicted probabilities appear to increase in general.

```{r, fig.cap = "Probabilities against Margin",echo=F}
boxplot(probabilities ~ newMammo$Margin)
```

Here we see that as the margin tends from being well-defined to ill-defined, in general, the probability of the tumor being malignant seems to increase.


Having observed these relationships, we can now predict the probability of the tumor being malignant for a few specific patients.
We do so for a patient at an age of 40, with Shape = 1 (round) and Margin = 1 (circumscribed). This is a patient which we would expect to have a relatively low probability, as they are quite young, and their tumor is quite regular in shape and margin.

The predicted probability is given by:

```{r,echo=F}
predict(final.glm,data.frame(Age=40,Shape="1",Margin="1"), type = "response")
```

This probability of 0.07147 alligns well with what we would expect. We can interpret this to mean that from a large group of patients, those with an age of 40, round tumors and circumscribed margins, approximately 7% would have malignant tumors.


On the other end of the spectrum, we can predict the probability for a patient with an age of 80, Shape = 4 (irregular) and Margin = 5 (speculated), that is an older patient with an irregular and very much ill-defined tumor.

The predicted probability is given by:

```{r,echo=F}
predict(final.glm,data.frame(Age=80,Shape="4",Margin="5"), type = "response")
```

This probability of 0.9461 also alligns well with what we would expect. This means, that for a large group of patients, we would expect that for patients of age 80, with irregularly shaped tumors and a spiculated margins, that approximately 95% would have malignant tumors.


To find what we would expect to be a more intermediate probability, we can then predict the probability for a patient with an age of 60, Shape = 2 (oval), Margin = 3 (obscured):

```{r,echo=F}
predict(final.glm,data.frame(Age=60,Shape="2",Margin="3"), type = "response")
```

This probability of 0.3381 also makes sense intuitively, as the patient's age and margin values were much more intermediate, and the shape of their tumor is closer to regular end of the spectrum than the irregular end. This means, that in a large group of patients, we would expect that for patients of age 60, with oval shaped tumors and obscured margins, that approximately 34% would have malignant tumors.


\newpage

#Part C

Our group was comprised of five members and we commenced the project after our first face-to-face meeting on the fourth of May. During the first meeting, we decided that we would all attempt the project questions on our own first because we value the project as a good opportunity to learn and practice the course material. We planned that we would then come together with our answers to confirm that we were on the same page and identify any mistakes that we may have made. 

Our initial plan was to meet one week after the first meeting, during which time we would try our best to finish part A individually. Then in the second week (13/05-19/05) we would assign one or two group members to write up part A, based on the answers we all agree upon in the second meeting. At this point we started to consider how to approach part B. Adopting a similar timeline as for part A, we decided to meet on the Friday of the second week to discuss the part B. In doing this, we would ensure that the bulk of the project was completed quite early, so that we would have ample time to format our write-up and ask questions about aspects about which we were unsure. Moreover, throughout the group project, if anyone had any problem, it was sent to the group chat and whoever was able to answer it provided some help.

However, when we started, we soon realised that some of the lecture content required for the project had not yet been covered, but we still tried to adhere to the planned timeline as much as possible. Part A was finished during the first week, other than question 6. In the second face-to-face meeting on 11th of May we had mutually agreed answers to all the other questions and a draft of the formal write-up of part A was allocated to Xiaomeng and Wenjun, who were to then send it to Anthony and Oliver to format it in Rmarkdown. The common questions we had were solved by going to both of the consulting times each week. Not all of our group members were available during those two time slots, but at least one member would go with questions prepared by the group and sent the feedback to the group chat afterwards. After the meeting on the first of June, all of us were confident with our answers for part B, and the write-up of part B was undertaken by Oliver and Anthony. During the entire period, we discussed the project in a Facebook group chat and in person after the lectures and tutorials. Wenjun was responsible for writing the reflection for part C, based on minutes kept in the meetings by Mohammad.  We then met on the third of June to ensure everyone was happy to submit the project and to see if there were any possible further improvements. 

###Individual Contributions:

**Shingyan:**

**Mohammad:** I tried to focus more on the codding side of the project and tried to make the best of all the consulting times. I was going through all the coding sections and was making sure that I understand the concept. There were times that others were unable to make to the consulting times due to their schedules to ask what they were struggling to understand. Therefore, I was asking their questions, if I was unable to provide them with the solution and was passing them on to others.

**Xiaomeng:**

**Wenjun:** Other than attempting the project on my own, participating in all the meetings and discussion, I drafted the write-up of part A with Xiaomeng and was in charge of writing Part C. I was delegated to go to several consulting and give the feedbacks to the group members could not make it too. 

**Oliver:** As was the case for all group members, I attempted the majority of the project alone before we discussed answers as a group to determine what to include in the report. In the writing of the final report, my major roles were explaining the subspaces question, as well as collating and formatting the work into single polished document with Anthony. 



\newpage

# Appendix

Here, we display the code used to generate ouput and plots following the order of the report. In part B, we display the model selection process in full.


##Part A

#1. 

Reading data into R:
```{r, eval = F}
child<-read.table("Child_Height.txt",header=T)
```

Creating pairwise scatterplot:
```{r, eval = F}
pairs(~Height+Weight+Length,data=child)
```



#2. 

Fitting linear models:
```{r, eval = F}
lm1<-lm(Length~Height+Weight, data=child)
lm2<-lm(Length~Height, data=child)
lm3<-lm(Length~Weight, data=child)

```


#4. 

Residuals plots for lm1:
```{r, eval = F}
par(mfrow=c(2,2))
plot(lm1)
```

Individual residuals vs. predictors plots for lm1:
```{r, eval = F}
par(mfrow=c(1,2))
res1<-rstudent(lm1)
fit<-fitted(lm1)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")
```


Residuals plots for lm2:
```{r, eval = F}
par(mfrow=c(2,2))
plot(lm2)
```

Individual residuals vs. predictors plots for lm2:
```{r, eval = F}
par(mfrow=c(1,2))
res1<-rstudent(lm2)
fit<-fitted(lm2)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")
```


Residuals plots for lm3:
```{r, eval = F}
par(mfrow=c(2,2))
plot(lm3)
```

Individual residuals vs. predictors plots for lm3:
```{r, eval = F}
par(mfrow=c(1,2))
res1<-rstudent(lm3)
fit<-fitted(lm3)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")

```


#5.

Summary for lm1:
```{r, eval = F}
summary(lm1)
```


Summary for lm2:
```{r, eval = F}
summary(lm2)
```

Summary for lm3:
```{r, eval = F}
summary(lm3)
```



#6.

###(a)

Model matrices for lm2 and lm3: 
```{r, eval = F}
lm2 <- lm(Length ~ Height, data = child)
lm3 <- lm(Length ~ Weight, data = child)
M2 <- model.matrix(lm2)
M3 <-  model.matrix(lm3)
M2
M3
```

###(b)

Defining vector of ones, $\mathbf{x}_1$ and $\mathbf{x}_2$ for the Gram Schmidt Process, as well as the function for the norm of a vector:
```{r, eval = F}
one <- c(1,1,1,1,1,1,1,1,1,1,1,1) # intercept vector
x1 <- M2[,2] # vector of height values
x2 <- M3[,2] # vector of weight values
norm_vec <- function(x) sqrt(as.numeric(t(x) %*% x))
```

Finding an orthonormal basis for $\mathcal{L}_1$:
```{r, eval = F}
v1 <- one / norm_vec(one)
v2_ <- x1 - as.numeric((t(x1) %*% v1)) * v1
v2 <- v2_ / norm_vec(v2_)
v2
```

Finding an orthonormal basis for $\mathcal{L}_2$:
```{r, eval = F}
w1 <- one / norm_vec(one)
w2_ <- x2 - as.numeric((t(x2) %*% w1)) * w1
w2 <- w2_ / norm_vec(w2_)
w2
```


###(c)

Computing the angle between the two spaces in **(b)**:
```{r, eval = F}
inner <- t(v2) %*% (w2)
norm_v2 <- norm_vec(v2)
norm_w2 <- norm_vec(w2)
theta <- acos(inner/(norm_v2 * norm_w2))
theta
```

\newpage 

## Part B

##Data Entry and Cleaning

Entering the data and defining any values which are assigned question marks to be missing values:
```{r, eval = F}
mammo <- read.csv("mammo.txt", header=TRUE, na.strings = "?")
```

Removing BI.RADS from our analysis:
```{r, eval = F}
mammo <- dplyr::select(mammo, Age, Shape, Margin, Density, Severity)
```

Correlation matrix using corrplot package:
```{r, eval = F}
corMat<-cor(na.omit(dplyr::select(mammo, Age, Shape, Margin,Severity)))
corrplot(corMat, type="upper", order="hclust",tl.col="black",tl.srt = 45 )
```

Pairwise scatterplot for mammo data:
```{r, fig.cap = "Comparison Matrix for Mammo Data", warning = F, eval = F}
ggpairs(mammo)
```

Checking variable class:
```{r, eval = F}
str(mammo)
```

Assigning varaibles as factors:
```{r, eval = F}
mammo$Shape <- as.factor(mammo$Shape)
mammo$Margin <- as.factor(mammo$Margin)
mammo$Density <- as.factor(mammo$Density)
mammo$Severity <- as.factor(mammo$Severity)
```

Checking variable classes again:
```{r, eval = F}
str(mammo)
```


## Data Visualisations and Data Summaries

Producing summary statistics:
```{r, eval = F}
summary(mammo$Age)
summary(mammo$Shape)
summary(mammo$Margin)
summary(mammo$Density)
summary(mammo$Severity)
```


Boxplot of Age Against Severity:
```{r, fig.cap = "Boxplot of Age Against Severity", eval = F, warning = F}
plot(mammo$Age~mammo$Severity)
```

Boxplot of Age Against Shape:
```{r, fig.cap = "Boxplot of Age Against Shape", eval = F, warning = F}
plot(mammo$Age~mammo$Shape)
```

Boxplot of Age Against Margin:
```{r, fig.cap = "Boxplot of Age Against Margin", eval = F, warning = F}
plot(mammo$Age~mammo$Margin)
```

Boxplot of Age Against Density:
```{r, fig.cap = "Boxplot of Age Against Density", eval = F, warning = F}
plot(mammo$Age~mammo$Density)
```

Bar Graph of Severity Against Shape:
```{r, fig.cap = "Bar Graph of Severity Against Shape", eval = F, warning = F}
ggplot(mammo, aes(x=mammo$Severity))+geom_bar(aes(fill=mammo$Shape))
```

Bar Graph of Severity Against Margin:
```{r, fig.cap = "Bar Graph of Severity Against Margin", eval = F, warning = F}
ggplot(mammo, aes(x=mammo$Severity))+geom_bar(aes(fill=mammo$Margin))
```

Bar Graph of Severity Against Density:
```{r, fig.cap = "Bar Graph of Severity Against Density", eval = F, warning = F}
ggplot(mammo, aes(x=mammo$Severity))+geom_bar(aes(fill=mammo$Density))
```


## Model Fitting and Model Selection

Fitting full model:
```{r, warning = F}
full.glm <- glm(Severity ~ (Age+Shape+Margin+Density)^2, data = mammo, family = "binomial")
summary(full.glm)
```

Here we note that p-values are non-existent for several of the interaction terms, as such, we begin by removing the interaction between Margin and Density, then view the summary for the updated model:

```{r, warning = F}
back.glm <- update(full.glm, .~. - Margin:Density)
summary(back.glm)
```

Here we see that p-values are still non-existent for some levels of the interaction between Shape and Density, as such we remove this interaction from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Shape:Density)
summary(back.glm)
```

Now we can see that all of the terms have a valid p-value, and continue our selection process by removing the least statistically signicant terms. We see that Age:Margin5 has the highest p-value of 0.9898, and no other level of Age:Margin are significant, so it is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Age:Margin)
summary(back.glm)
```

Now we see that the highest p-value is for Shape2:Margin3, and no other levels of the interaction between shape and margin are significant, so the interaction is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Shape:Margin)
summary(back.glm)
```

Here we see that of the interaction terms, Age:Density4 has the highest p-value of 0.7959, and no other levels of this interaction are significant, so the model is updated with the removal of this interaction:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Age:Density)
summary(back.glm)
```

Here we see that Age:Shape3 has the highest p-value of 0.5688, and the other levels of the interaction between Age and Shape are also non-significant, so the interaction is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Age:Shape)
summary(back.glm)
```

Now we note that the model has been reduced to the additive model with no interaction terms. In this model we see that the fourth level of density has the highest p-value, and no other levels are significant, so Density is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Density)
summary(back.glm)
```

In this model, we note that all of the terms have at least on level which is statistically significant, so no terms should be removed. We assign this model the name final.glm:

```{r, warning = F}
final.glm <- back.glm
```

## Justification of the final model

Printing model summary:
```{r, eval = F}
summary(final.glm)
```

##Predicting Probabilities and Interpretation

Fitting probabilities from data:
```{r, eval = F}
probabilities <- fitted(final.glm)
summary(probabilities)
```

Producing histogram of probabilties:
```{r, fig.cap = "Histogram of Fitted Probabilities", eval = F}
hist(probabilities)
```

Assigning new version of mammo data with only Age, Margin, Shape and Density and no missing values:
```{r, eval = F}
newMammo <- mammo %>% select(Age, Margin, Shape, Severity)
newMammo <- na.omit(newMammo)
```

Plotting probabilties against Age:
```{r, fig.cap = "Probabilities against Age", eval = F}
plot(probabilities ~ newMammo$Age)
```

Plotting probabilties against Shape:
```{r, fig.cap = "Probabilities against Shape", eval = F}
boxplot(probabilities ~ newMammo$Shape)
```

Plotting probabilties against Margin:
```{r, fig.cap = "Probabilities against Margin", eval = F}
boxplot(probabilities ~ newMammo$Margin)
```

Predicting probability of malignant tumor for patient of Age = 40, Shape = 1, Margin = 1:
```{r, eval = F}
predict(final.glm,data.frame(Age=40,Shape="1",Margin="1"), type = "response")
```

Predicting probability of malignant tumor for patient of Age = 80, Shape = 4, Margin = 5:
```{r, eval = F}
predict(final.glm,data.frame(Age=80,Shape="4",Margin="5"), type = "response")
```

Predicting probability of malignant tumor for patient of Age = 60, Shape = 2, Margin = 3:
```{r,echo=F}
predict(final.glm,data.frame(Age=60,Shape="2",Margin="3"), type = "response")
```


