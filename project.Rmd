---
title: "SM3_Project"
author: "Shingyan Kwong"
date: "May 13, 2018"
output:
  pdf_document: default
  html_document: default
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
---
```{r,echo=FALSE}
library(broom)
library(MASS)
library(magrittr)
library(dplyr)
#setwd("C:/Users/Oliver/Documents/GitHub/Stat_Model_Project")
```



#Part A.

#1. 
```{r,echo=FALSE}
child<-read.table("Child_Height.txt",header=T)
```


```{r}
pairs(~Height+Weight+Length,data=child)
```

There is evidence of a strong, positive linear relationship between  length and the two predictor variables, height and weight. The associated correlation coefficients are 0.881 and 0.894 respectively. There is also a strong, positive linear relationship between height and weight. This suggests that the two predictors may be dependent one another.


#2. 

```{r}
lm1<-lm(Length~Height+Weight, data=child)
lm2<-lm(Length~Height, data=child)
lm3<-lm(Length~Weight, data=child)

```


#3. 

The model assumptions which may be checked via diagnostic plots are as follows. 

Linearity: Check the residuals vs fitted and the residuals vs predictor plots. Linearity is reasonable if random scatter above and below the 0 line is observed. 

Constant Variance: Check scale location plot. Homoscedacity is reasonable if constant variance of residuals is observed across the scale location plot. 

Normality: Check normal qq plot. Normality is reasonable if most points between -2 and 2 are on/close to the diagonal line. 

#4. 

#Full model

```{r}
par(mfrow=c(2,2))
plot(lm1)
```

```{r}
par(mfrow=c(1,2))
res1<-rstudent(lm1)
fit<-fitted(lm1)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")

```


Linearity: Given the small number of data points available, roughly random scatter is observed in the residual vs fitted and residual vs predictor plots. There is a couple of high residual points but it is not too bad. Linearity is reasonable. 

Constant variance: Scale location plots appear to show heteroscedacity. Constant variance is not reasonable.  

Normality: There is some minor departure from normality in the beginning and the tails of the standardized residuals. Overall the points are fairly close to the diagonal line. Normality is reasonable. 

Leverage: There are 2 data points in the zone of danger. These high leverage points are having a disproportionate effect on the model. 

#Model with Height only

```{r}
par(mfrow=c(2,2))
plot(lm2)
```

```{r}
par(mfrow=c(1,2))
res1<-rstudent(lm2)
fit<-fitted(lm2)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")
```


Linearity: Non-random scatter observed in residual vs fitted and residual vs predictor plots. Linearity is not reasonable. 

Constant Variance: Variance appears to increase for the middle fitted values and then decrease again. Constant variance is not reasonable. 

Normality: There are several points deviating from the diagonal line on the normal qq plot. Normality is not reasonable. 

Leverage: There is one point with high leverage. 

#Model with Weight only

```{r}
par(mfrow=c(2,2))
plot(lm3)
```

```{r}
par(mfrow=c(1,2))
res1<-rstudent(lm3)
fit<-fitted(lm3)
plot(child$Height,res1,main="Residuals vs height",pch=20)
abline(0,0,col="red")
plot(child$Weight,res1,main="Residuals vs weight",pch=20)
abline(0,0,col="red")

```

Linearity: Rough random scatter in observed in residual vs fitted and residual vs predictor plots. The fitted plot shows some evidence of curvature but overall it is acceptable. Linearity is reasonable. 

Constant Variance: Variance is roughly constant across the scale location plot. Constant variance is reasonable. 

Normality: Most points are close to the diagonal line except 2. Normality is reasonable. 

Leverage: There is one data point with high leverage. 

#5. Comparison of the three models

Full Model:
```{r}
summary(lm1)
```

Height Model:

```{r}
summary(lm2)

```

Weight Model:
```{r}
summary(lm3)

```

(a)

In the full model, neither predictor variables is statistically significant (at the 0.05 level), and the numerical values of the two coefficients are both smaller than those of the single predictor models.

(b)

Full model:

Holding height constant, the full model predicts that an increase of of 1kg will on average increase the length of the cathetar by 0.42081cm. 

Weight only model:

Without regard for height, this model predicts that an increase of 1kg will on average increase the cathetar length by 0.61136cm. 

#6

###(a) We construct the model matrices for the height only and weight only models. 

```{r,echo=FALSE}
lm2 <- lm(Length ~ Height, data = child)
lm3 <- lm(Length ~ Weight, data = child)
M2 <- model.matrix(lm2)
M3 <-  model.matrix(lm3)
M2
M3
```

Here, we define $\mathbf{1} := (1,1,1,1,1,1,1,1,1,1,1,1)$, the vector of intercepts for both models. We also denote the vector of height values by $\mathbf{x}_1$ and the vector of weight values by $\mathbf{x}_2$. Then we find that:

$\mathcal{L}_1$ is the space spanned by the columns of M2, that is $\mathcal{L}_1 = span \{ \mathbf{1}, \mathbf{x}_1 \}$

$\mathcal{L}_2$ is the space spanned by the columns of M3, that is $\mathcal{L}_2 = span \{ \mathbf{1}, \mathbf{x}_2 \}$

Then, the intersection of the two subspaces is the intercept column, that is $\mathcal{L}_1 \cap \mathcal{L}_2 = span { \{\mathbf{1} \} }.$

###(b)

We note that $(\mathcal{L}_1 \cap \mathcal{L}_2)^{\perp}$ is the subspace of all vectors orthogonal to $\mathbf{1}$. Then, in order to find the intersections of $\mathcal{L}_1$ and $\mathcal{L}_2$ with $(\mathcal{L}_1 \cap \mathcal{L}_2)^{\perp}$, we first find orthonormal bases for $\mathcal{L}_1$ and $\mathcal{L}_2$.

We achive this by applying the Gram-Schmidt process. First, we define the basis vectors for both subspaces, and a function norm_vec to find the norm of a vector: 

```{r}
one <- c(1,1,1,1,1,1,1,1,1,1,1,1) # intercept vector
x1 <- M2[,2] # vector of height values
x2 <- M3[,2] # vector of weight values
norm_vec <- function(x) sqrt(as.numeric(t(x) %*% x))
```

Next, we find an orthonormal basis for $\mathcal{L}_1$:

```{r}
v1 <- one / norm_vec(one)
v2_ <- x1 - as.numeric((t(x1) %*% v1)) * v1
v2 <- v2_ / norm_vec(v2_)
```

Then $\mathcal{L}_1 = span \{\mathbf{v}1, \mathbf{v}2\}$.

Now, an orthonormal basis for $\mathcal{L}_2$:

```{r}
w1 <- one / norm_vec(one)
w2_ <- x2 - as.numeric((t(x2) %*% w1)) * w1
w2 <- w2_ / norm_vec(w2_)
```

Then $\mathcal{L}_2 = span \{\mathbf{w}1, \mathbf{w}2\}$.

Now, we have that $\mathbf{v}_1 = \mathbf{w}_1$ is parallel to $\mathbf{1}$, and that $\mathbf{v}_2$ and $\mathbf{w}_2$ are orthogonal to $\mathbf{1}$, that is, $\mathbf{v}_2, \mathbf{w}_2 \in ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$.

As a result, we find that:

$$ \mathcal{L}_1 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp} = span \{ \mathbf{v}_2 \}; $$

$$ \mathcal{L}_2 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp} = span \{ \mathbf{w}_2 \}. $$



###(c)

Given that both $\mathcal{L}_1 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$ and $\mathcal{L}_2 \cap ({\mathcal{L}_1 \cap \mathcal{L}_2})^{\perp}$ are one-dimensional subspace, we can compute the angle between them using the relation:

$$
\cos \theta = \frac{ \langle \mathbf{u}, \mathbf{v} \rangle }{||\mathbf{u}|| \; ||\mathbf{v}||}
$$

Where $\mathbf{u}$ and $\mathbf{v}$ are two vectors and $\theta$ is the angle between them.

We compute the angle between the two spaces in **(b)** as follows:

```{r}
inner <- t(v2) %*% (w2)
norm_v2 <- norm_vec(v2)
norm_w2 <- norm_vec(w2)
theta <- acos(inner/(norm_v2 * norm_w2))
theta
```

The angle is not $\pi$ indicating that the two spaces are not orthogonal. This suggests that height and weight are not independent. In fact they are fairly correlated. 

#7

Picking a model. 

_______

Comparing these three models, lm3 (length~weight) is better than others. From the analysis of diagnostic plots of these three models, the four assumptions in lm3 can be considered as the most reasonable. Moreover, the angle between two spaces is approximately equal to two, which means they are not orthogonal to each other. Furthermore, there exists linear relationship between the two predictor variables (height and weight) with correlation 0.961. Overall, weight as the predictor variable and length as the response variables is the most appropriate model.

______


\newpage

# Part B


## Introduction

In this section we obtain a predictive model for mammographic mass severity, a measure of the status of mammographic mass lesions, on a scale from 0 to 1, where 0 is assigned to a benign tumor, and 1 is assigned to a malignant tumor. Interest in this analysis arises from there being a low predicitve value of breast biopsy from mammograms. This low predictive value has been found to lead to approximately 70% of unnessessary biopsies of benign tumors. Analysis is performed on the dataset "mammo", containing the true status of 961 mammographic mass lesions, with the response variable severity as described. Four response variables are considered:

**Age** - the patient's age in years;

**Shape** - a factor variable with four levels: 1 for round, 2 for oval, 3 for lobular, and 4 for irregular;

**Margin** - a factor varaible with five levels: 1 for circumscribed, 2 for microlobulated, 3 for obscured, 4 for ill-defined, and 5 for spiculated;

**Density** - a factor with four levels: 1 for high, 2 for iso, 3 for low, and 4 for fat-containing.


**This introduction should probably be reworked but I this hope is a good starting point**


##Data Entry and Cleaning
First, we enter the data and define any values which are assigned question marks to be missing values:

```{r}
mammo <- read.csv("mammo.txt", header=TRUE, na.strings = "?")
```

We then note that BI.RADS is not a predictor variable, and remove it from our analysis:

```{r}
mammo <- dplyr::select(mammo, Age, Shape, Margin, Density, Severity)
```

We generate a correlation matrix to observe any relationships between the predictor variables and the response. It was necessary to omit observations with NA values. Shape and Margin were included as increases in their indices are associated with a greater risk of cancer. Density was omitted as the index is not associated with a greater risk. Thus correlation between Density and Severity would not be statistically meaningful.  

```{r}
library(corrplot)
corMat<-cor(na.omit(dplyr::select(mammo, Age, Shape, Margin,Severity)))
corrplot(corMat, type="upper", order="hclust",tl.col="black",tl.srt = 45 )
```

Mild positive correlation is observed between Age and the other variables. 

Moderate positive correlation is observed between Severity and the other 3 predictors. 

Strong positive correlation between Shape and Margin. 




We can now check the variable types for the data:

```{r}
str(mammo)
```

We note that Shape, Margin, Density and Severity should all be factor variables, and as such convert them:

```{r}
mammo$Shape <- as.factor(mammo$Shape)
mammo$Margin <- as.factor(mammo$Margin)
mammo$Density <- as.factor(mammo$Density)
mammo$Severity <- as.factor(mammo$Severity)
```

We now see that all of the data types are correct:

```{r}
str(mammo)
```


## Data Visualisations and Data Summaries

To visualise the data, we first produce summary statisitics for the dataset as a whole, and for each individual variable:

```{r}
summary(mammo$Age)
print(" ")
summary(mammo$Shape)
print(" ")
summary(mammo$Margin)
print(" ")
summary(mammo$Density)
print(" ")
summary(mammo$Severity)
```

We also create a pairwise scatterplot to observe the relationships between individual variables:

```{r, fig.cap = "Pairwise scatterplot of Mammographic Mass Severity Data"}
pairs(mammo)
```

There appears to be a weak,possibly linear, positive relationship between Age and Severity. There are no observable relationships between Severity and the other predictors. 


## Model Fitting and Model Selection

We now fit a logistic linear model (full.glm) to the data, with Severity as the response variable, and Age, Shape, Margin and Density as the predictor variables with interaction terms up to second order:

```{r, warning = F}
full.glm <- glm(Severity ~ (Age+Shape+Margin+Density)^2, data = mammo, family = "binomial")
summary(full.glm)
```

Here we note that p-values are non-existent for several of the interaction terms, as such, we begin by removing the interaction between Margin and Density, then view the summary for the updated model:

```{r, warning = F}
back.glm <- update(full.glm, .~. - Margin:Density)
summary(back.glm)
```

Here we see that p-values are still non-existent for some levels of the interaction between Shape and Density, as such we remove this interaction from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Shape:Density)
summary(back.glm)
```

Now we can see that all of the terms have a valid p-value, and continue our selection process by removing the least statistically signicant terms. We see that Age:Margin5 has the highest p-value of 0.9898, and no other level of Age:Margin are significant, so it is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Age:Margin)
summary(back.glm)
```

Now we see that the highest p-value is for Shape2:Margin3, and no other levels of the interaction between shape and margin are significant, so the interaction is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Shape:Margin)
summary(back.glm)
```

Here we see that of the interaction terms, Age:Density4 has the highest p-value of 0.7959, and no other levels of this interaction are significant, so the model is updated with the removal of this interaction:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Age:Density)
summary(back.glm)
```

Here we see that Age:Shape3 has the highest p-value of 0.5688, and the other levels of the interaction between Age and Shape are also non-significant, so the interaction is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Age:Shape)
summary(back.glm)
```

Now we note that the model has been reduced to the additive model with no interaction terms. In this model we see that the fourth level of density has the highest p-value, and no other levels are significant, so Density is removed from the model:

```{r, warning = F}
back.glm <- update(back.glm, .~. - Density)
summary(back.glm)
```

In this model, we note that all of the terms have at least on level which is statistically significant, so no terms should be removed. We assign this model the name final.glm:

```{r, warning = F}
final.glm <- back.glm
```

## Justification of the final model

The following is the proposed final model. 

```{r}
summary(final.glm)
```

This model was obtained by first starting with a saturated model with all the two way interaction terms. Statistically insiginificant terms were removed via the backwards selection algorithmn at the 0.05 significance level. Hence the final model is the most parsimonious model with all the statistically significant predictors. 

## Interpretation of Parameters

Intercept: A woman of Age=0 with a mammogram of Shape=1 (round), Margin=1 (circumscribed) has log-odds=-4.7195 of having a malignant tumour. 

Age: Holding all other variables constant, a one year increase in age increases the log-odds of having a malignant tumour by 0.05388. 

Shape: Holding all other variables constant, having a lesion of Shape=2 (oval) increases the log-odds of having a malignant tumour by -.4478 compared to Shape=1 (round). For Shape=2 (lobular), th increase is 0.4992 and for Shape=3 (irregular), the increase is 1.2428, all relative to Shape=1.  

Margin: Holding all other variables constant, having a lesion of Margin=2 (microlobulated) increases the log-odds of a malignant tumour by 1.5829 comapred to Margin=1 (circumscribed). For Margin =3 (obscured) the change in log-odds is 1.2631, Margin=4 (ill-defined) 1.5432 and Margin=5 (spiculated) 2.0321, all relative to Margin=1. 


##Predicting Probabilities and Interpretation

In this section, we use our final model (final.glm) to predict the probability of a specific patient, that is, a patient with given values for each of the predictor varaibles. Given that the response variable is defined to be 0 for benign (not cancerous) and 1 for malignant (cancerous), the fitted values lie between 0 and 1 and hence predict the  probability for a given patient to have a malignant tumor.

We first fit the probabilties of each datapoint in the dataset based on the final model:

```{r}
probabilities <- fitted(final.glm)
summary(probabilities)
```

We can produce a histogram to visualise the overall distribution of probabilities:

```{r, fig.cap = "Histogram of Fitted Probabilities"}
hist(probabilities)
```

Here we note that in general, it appears that most patients are either very likely, or very unlikely to have a malignant tumor. As a result, we might expect when predicting probabilities, that is most cases the predictions will be either very high or very low.

We can produce plots to visualise the probabilities for different levels of the predictor variables.

We first define a modified version of the mamo data, including only Age, Shape, Margin and Severity, and ignoring the missing values in order to be able to create valid plots:

```{r}
newMammo <- mammo %>% select(Age, Margin, Shape, Severity)
newMammo <- na.omit(newMammo)
```


We can now create plots of probabilities against Age, Shape and Margin:

```{r, fig.cap = "Probabilities against Age"}
plot(probabilities ~ newMammo$Age)
```

Here we see that there appears to be a weak, positive relationship between age and the probability of having a malignant tumor, and it is difficult to say whether the relationship is linear or not.

```{r, fig.cap = "Probabilities against Shape"}
boxplot(probabilities ~ newMammo$Shape)
```

Here we see that as shape tends from the round, regular shape to a more irregular one, the predicted probabilities appear to increase in general.

```{r, fig.cap = "Probabilities against Margin"}
boxplot(probabilities ~ newMammo$Margin)
```

Here we see that as the margin tends from being well-defined to ill-defined, in general, the probability of the tumor being malignant seems to increase.


Having observed these relationships, we can now predict the probability of the tumor being malignant for a few specific patients.
We do so for a patient at an age of 40, with Shape = 1 (round) and Margin = 1 (circumscribed). This is a patient which we would expect to have a relatively low probability, as they are quite young, and their tumor is quite regular in shape and margin.

The predicted probability is given by:

```{r}
predict(final.glm,data.frame(Age=40,Shape="1",Margin="1"), type = "response")
```

This probability of 0.07147 alligns well with what we would expect. We can interpret this to mean that from a large group of patients, those with an age of 40, round tumors and circumscribed margins, approximately 7% would have malignant tumors.


On the other end of the spectrum, we can predict the probability for a patient with an age of 80, Shape = 4 (irregular) and Margin = 5 (speculated), that is an older patient with an irregular and very much ill-defined tumor.

The predicted probability is given by:

```{r}
predict(final.glm,data.frame(Age=80,Shape="4",Margin="5"), type = "response")
```

This probability of 0.9461 also alligns well with what we would expect. This means, that for a large group of patients, we would expect that for patients of age 80, with irregularly shaped tumors and a spiculated margins, that approximately 95% would have malignant tumors.


To find what we would expect to be a more intermediate probability, we can then predict the probability for a patient with an age of 60, Shape = 2 (oval), Margin = 3 (obscured):

```{r}
predict(final.glm,data.frame(Age=60,Shape="2",Margin="3"), type = "response")
```

This probability of 0.3381 also makes sense intuitively, as the patient's age and margin values were much more intermediate, and the shape of their tumor is closer to regular end of the spectrum than the irregular end. This means, that in a large group of patients, we would expect that for patients of age 60, with oval shaped tumors and obscured margins, that approximately 34% would have malignant tumors.


